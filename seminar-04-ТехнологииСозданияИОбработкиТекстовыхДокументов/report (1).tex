\documentclass{ou}
\begin{document}
\thispagestyle{empty}
\begin{center}
\minobrRF\\
\FGAVO\\
\textbf{\sixteen{\university}}\\
\institute\\
\department\\[1\baselineskip]
{\fontsize{16}{24}\selectfont\bfseries \work}\\[0.5\baselineskip]
\discipline\\
ВАША ТЕМА\\[1\baselineskip]
\end{center}
\instructor \hfill \selectfont \Nigmatullin\\[1\baselineskip]
\students \hfill \selectfont \group\\
\vfill
\begin{center}
    \city
\end{center}
\newpage
\tableofcontents
\newpage
\section{Представление и кодирование текстовых данных в цифровой форме}
\subsection{Кодирование текстовой, звуковой и графичекой информации}
Для того чтобы весь мир одинаково кодировал текстовые данные, нужны единые таблицы кодирования. Институт стандартизации США ввел в действие систему кодирования ASCII (American Standard Code for Information Interchange – стандартный код информационного обмена США). В системе ASCII закреплены две таблицы кодирования – базовая и расширенная. Базовая таблица закрепляет значения кодов от 0 до 127, а расширенная относится к символам с номерами от 128 до 255. Первые 32 кода базовой таблицы, начиная с нулевого, отданы производителям аппаратных средств (в первую очередь производителям компьютеров и печатающих устройств). В этой области размещаются так называемые управляющие коды, которым не соответствуют никакие символы. Начиная с кода 32 по код 127 размещены коды символов английского алфавита, знаков препинания, цифр, арифметических действий и некоторых вспомогательных символов. 

Расширенная часть системы кодирования, определяющая значения кодов со 128 по 255, – это национальная система кодирования. Отсутствие единого стандарта в этой области привело к множественности одновременно действующих кодировок. Разнообразие кодировок вызвано ограниченным набором кодов (256) при восьмиразрядном кодировании. Универсальная система кодирования UNICODE, основанная на 16-разрядном кодировании символов, позволяет обеспечить уникальные коды для 65 536 различных символов – этого поля достаточно для размещения в одной таблице символов большинства языков планеты. В системе кодирования UNICODE все текстовые документы автоматически становятся вдвое длиннее по сравнению с 8-разрядным кодированием. 

В зависимости от способа формирования изображений компьютерную графику принято подразделять на растровую (графический объект представлен в виде комбинации точек, образующих растр и обладающих свойствами яркости и цвета), векторную (элементарным объектом является не точка, а линия) и фрактальную (базовым элементом является математическая формула). Трехмерная (3D) графика сочетает в себе векторный и растровый способ формирования изображений. Поскольку линейные координаты и индивидуальные свойства каждой точки (яркость) можно выразить с помощью целых чисел, то можно сказать, что растровое кодирование позволяет использовать двоичный код для представления графических данных. Кодирование черно-белых изображений осуществляется восьмиразрядным кодированием, что позволяет отобразить 256 градаций серого цвета. Для кодирования цветных графических изображений применяется принцип декомпозиции произвольного цвета на основные составляющие. В качестве таких составляющих используют три основных цвета: красный (Red, R), зеленый (Green, G) и синий (Blue, B). Такая система кодирования называется системой RGB. Если для кодирования яркости каждой из основных составляющих использовать по 256 значений (8 двоичных разрядов), то на кодирование цвета одной точки надо затратить 24 разряда, что обеспечивает определение 16,5 млн различных цветов. Такой режим называется полноцветным (True Color). Каждому из основных цветов можно поставить в соответствие дополнительный цвет, то есть цвет, дополняющий основной цвет до белого. Для любого из основных цветов дополнительным будет цвет, образованный суммой пары остальных основных цветов. Данная система кодирования обозначается четырьмя буквами CMYK и для представления цветной графики в этой системе надо иметь 32 двоичных разряда. Такой режим тоже называется полноцветным (True Color). 

В кодировании звуковой информации можно выделить два основных направления. Метод FM (Frequency Modulation) основан на том, что теоретически любой сложный звук можно разложить на последовательность простейших гармонических сигналов разных частот, каждый из которых представляет собой правильную синусоиду, а следовательно, может быть описан числовыми параметрами, т.е. кодом. Разложение звуковых сигналов в гармонические ряды и представление в виде дискретных цифровых сигналов выполняют специальные устройства – аналого-цифровые преобразователи (АЦП). Обратное преобразование для воспроизведения звука, закодированного числовым кодом, выполняют цифроаналоговые преобразователи (ЦАП). При таких преобразованиях неизбежны потери информации, поэтому качество звукозаписи не вполне удовлетворительное. В то же время данный метод кодирования обеспечивает весьма компактный код. Метод таблично-волнового (Wave-Table) синтеза лучше соответствует современному уровню развития техники. Проще говоря, где-то в заранее подготовленных таблицах хранятся образцы звуков (сэмплы) для множества различных музыкальных инструментов. Числовые коды выражают тип инструмента, номер его модели, высоту тона, продолжительность и интенсивность звука, динамику его изменения и другие показатели.
\newpage
\subsection{Unicode}
Unicode определяют несколько форм кодирования из своего общего репертуара: UTF-8, UCS-2, UTF-16, UCS-4 и UTF-32. В форме кодирования каждый символ представлен одной или несколькими единицами кодирования. Все стандартные формы кодирования UCS, кроме UTF-8, имеют единицу кодирования, превышающую один октет, что затрудняет их использование во многих современных приложениях и протоколах, предполагающих использование 8- или даже 7-битных символов.

UTF-8, имеет единицу кодирования, равную одному октету. Он использует все биты октета, но при этом сохраняет полную совместимость с US-ASCII диапазон: символы US-ASCII кодируются одним октетом с обычным значением US-ASCII, и любой октет с таким значением может обозначать только символ US-ASCII и ничего больше. UTF-8 кодирует символы UCS с помощью разного количества октетов, где количество октетов и значение каждого из них зависят от целочисленного значения, присвоенного символу в стандарте ISO/IEC 10646 (номер символа, также известный как кодовая позиция, кодовая точка или скалярное значение Unicode).

Эта форма кодирования имеет следующие характеристики: Номера символов от U+0000 до U+007F (репертуар US-ASCII) соответствуют октетам от 00 до 7F (7-битным значениям US-ASCII). Прямым следствием этого является то, что обычная строка ASCII также является допустимой строкой UTF-8 строкой. US-Значения октетов ASCII иначе не отображаются в кодировке UTF-8 поток символов. Это обеспечивает совместимость с файловыми системами или другим программным обеспечением (например, функцией printf() в библиотеках C), которые анализируются на основе значений US-ASCII, но прозрачны для других значений 

 Преобразование в обе стороны легко между UTF-8 и другими формами кодировки. Первый октет многооктетной последовательности указывает на количество октетов в последовательности. Значения октетов C0, C1, F5 и FF никогда не встречаются. Границы символов легко определить в любом месте потока октетов. Лексикографический порядок сортировки строк UTF-8 по значениям байтов такой же, как при сортировке по номерам символов. Конечно, это представляет ограниченный интерес, поскольку порядок сортировки, основанный на номерах символов, почти никогда не соответствует культурным особенностям.
 
  Алгоритм быстрого поиска Бойера — Мура можно использовать с данными в кодировке UTF-8. Строки в кодировке UTF-8 можно довольно надёжно распознать с помощью простого алгоритма, то есть вероятность того, что строка символов в любой другой кодировке будет распознана как допустимая UTF-8, невелика. UTF-8 был разработан в сентябре 1992 года Кеном Томпсоном в соответствии с критериями проектирования, указанными Робом Пайком, с целью определения формата преобразования UCS, который можно было бы использовать в операционной системе Plan9 без каких-либо изменений. Проект Томпсона был стандартизирован Объединённой группой по интернационализации X/Open XOJIG.
  
Условные обозначения: Ключевые слова «ДОЛЖЕН», «НЕ ДОЛЖЕН», «ОБЯЗАН», «СЛЕДУЕТ», «СЛЕДУЕТ НЕ СЛЕДУЕТ», «СЛЕДУЕТ», «СЛЕДУЕТ НЕ СЛЕДУЕТ», «РЕКОМЕНДУЕТСЯ», «МОЖЕТ» и «НЕОБЯЗАТЕЛЬНО» в этом документе следует интерпретировать так, как описано в RFC2119.

 Символы UCS обозначаются как U+HHHH, где HHHH — это строка из 4–6 шестнадцатеричных цифр, представляющая номер символа в стандарте ISO/IEC 10646.
 
UTF-8 определяется стандартом Unicode. Описание и формулы также можно найти в приложении D стандарта ISO/IEC 10646-1. В UTF-8 символы из диапазона U+0000...U+10FFFF (доступный диапазон UTF-16) кодируются с помощью последовательностей из 1–4 октетов. В единственном октете «последовательности» старший бит равен 0, а остальные 7 бит используются для кодирования номера символа. В последовательности из n октетов, n>1, начальный октет имеет n битов более высокого порядка значение 1, за которым следует бит, равный 0. Оставшиеся биты этого октета содержат биты из номера символа, подлежащего кодированию. Все следующие октеты имеют бит старшего порядка, установленный в 1, а следующий бит установлен в 0, оставляя по 6 бит в каждом, чтобы содержать биты из кодируемого символа.

Кодирование символа в UTF-8 происходит следующим образом:

 1. Определите количество октетов, необходимых для кодирования символа, по его номеру и первому столбцу приведённой выше таблицы. Важно отметить, что строки таблицы являются взаимоисключающими, то есть существует только один допустимый способ кодирования данного символа.
 
 2. Подготовьте старшие биты октетов в соответствии со вторым столбцом таблицы. 
 
 3. Заполните биты, отмеченные x, битами номера символа, выраженного в двоичной системе. Начните с того, что поместите младший бит номера символа в младший бит последнего октета последовательности, затем поместите следующий старший бит номера символа в следующий старший бит этого октета и т. д. Когда x битов последнего октета будут заполнены, перейдите к предпоследнему октету, затем к предыдущему и т. д., пока не будут заполнены все x битов. 
 
Определение UTF-8 запрещает кодирование номеров символов в диапазоне от U+D800 до U+DFFF, которые зарезервированы для использования с формой кодирования UTF-16 (в виде суррогатных пар) и не представляют непосредственно символы. При кодировании данных в формате UTF-8 из формата UTF-16 необходимо сначала декодировать данные в формате UTF-16, чтобы получить номера символов, которые затем кодируются в формате UTF-8, как описано выше. Это отличается от CESU-8, которая представляет собой кодировку, подобную UTF-8, но не предназначенную для использования в Интернете. CESU-8 работает аналогично UTF-8, но кодирует значения кода UTF-16 (16-битные величины) вместо номера символа (кодовой точки). Это приводит к разным результатам для номеров символов выше 0xFFFF; кодировка этих символов в CESU-8 НЕ является допустимой кодировкой UTF-8.

Синтаксис байтовых последовательностей UTF-8

 Для удобства разработчиков, использующих ABNF, здесь приводится определение UTF-8 в синтаксисе ABNF. Строка UTF-8 — это последовательность октетов, представляющая собой последовательность символов UCS. Последовательность октетов является допустимой UTF-8, только если она соответствует следующему синтаксису, который основан на правилах кодирования UTF-8 и выражен в ABNF.
 
Метка порядка байтов (BOM) Символ UCS U+FEFF «НЕРАЗРЫВНЫЙ ПРОБЕЛ НУЛЕВОЙ ШИРИНЫ» также известен как «МЕТКА ПОРЯДКА БАЙТОВ» (сокращённо «BOM»). Этот символ может использоваться в тексте как настоящий «пробел без переноса строки», но название спецификации указывает на второе возможное применение этого символа: добавление символа U+FEFF в начало потока символов UCS в качестве «подписи». Получатель такого сериализованного потока может затем использовать начальный символ как подсказку о том, что поток состоит из символов UCS, а также для распознавания используемой кодировки UCS и, с кодировками, имеющими многооктетную единицу кодирования, как способ.

В UTF-8 используется однооктетная единица кодирования, эта последняя функция бесполезна, и спецификация всегда будет отображаться как последовательность октетов EF BB BF.Важно понимать, что символ U + FEFF, появляющийся в любой позиции, отличной от начала потока, ДОЛЖЕН интерпретироваться в соответствии с семантикой неразрывного пробела нулевой ширины и ДОЛЖЕН НЕ может быть истолковано как подпись. При интерпретации как подписи стандарт Юникода предполагает, что начальный символ U+FEFF может быть удалён перед обработкой текста. Такое удаление необходимо в некоторых случаях (например, при объединении двух строк, поскольку в противном случае результирующая строка может содержать непреднамеренный символ «ПРОБЕЛ БЕЗ ПЕРЕНОСА » в месте соединения), но может повлиять на внешний процесс на другом уровне (например, на цифровую подпись или подсчёт символов), который зависит от наличия всех символов в потоке. Поэтому РЕКОМЕНДУЕТСЯ не удалять начальный символ.

 U+FEFF, который интерпретируется как подпись, без веской причины, а игнорировать его вместо того, чтобы удалять его, когда это уместно (например, для отображения), удаляйте его только в случае крайней необходимости. U+FEFF в первой позиции потока МОЖЕТ интерпретироваться как неразрывный пробел нулевой ширины и не всегда является сигнатурой. В попытка уменьшить эту неопределенность, Юникод 3.2 добавляет новый символ, символ U+2060 "слово столяр", ровно с той же семантикой и использование как U+FЭФФ кроме подписи функции, и настоятельно рекомендует ее использование исключительно для выражения слов-присоединение семантики. В конечном счете, следование этой рекомендации сделает все, кроме уверенности в том, что любой начальный U + FEFF является подписью, а не предполагаемым "ПРОБЕЛОМ БЕЗ РАЗРЫВА НУЛЕВОЙ ШИРИНЫ".
 
 Тем временем неопределённость, к сожалению, сохраняется и может повлиять на интернет-протоколы. Спецификации протоколов МОГУТ ограничивать использование U+FEFF в качестве подписи, чтобы уменьшить или устранить возможные негативные последствия этой неопределённости. В интересах достижения баланса между преимуществами (уменьшение неопределённости) и недостатками (потеря функции подписи) таких ограничений полезно выделить несколько случаев: 
 
Протокол должен запрещать использование U+FEFF в качестве подписи для тех текстовых элементов протокола, которые, согласно протоколу, всегда должны быть  в кодировке UTF-8, поскольку в таких случаях функция подписи совершенно бесполезна. Протокол ТАКЖЕ ДОЛЖЕН запрещать использование U+FEFF в качестве подписи для тех текстовых элементов протокола, для которых протокол предусматривает механизмы идентификации кодировки символов, если ожидается, что реализации протокола всегда будут использовать эти механизмы должным образом. Так будет, когда элементы протокола находятся под строгим контролем реализации с момента их создания до момента их (правильно помеченных) передачи.

Протокол не должен запрещать использование U+FEFF в качестве подписи для тех текстовых элементов протокола, для которых протокол не предусматривает механизмы идентификации кодировки символов, когда запрет будет невозможно обеспечить или когда ожидается, что реализации протокола не всегда смогут правильно использовать эти механизмы. Последние два случая вероятны при работе с более крупными элементами протокола, такими как объекты MIME, особенно когда реализации протокола получают такие объекты из файловых систем, из протоколов, в которых нет механизмов идентификации кодировки полезных данных (например, FTP), или из других протоколов, которые не гарантируют правильную идентификацию кодировки символов (например, HTTP). 

Если протокол запрещает использование U+FEFF в качестве подписи для определенного элемента протокола, то любое начальное U+FEFF в этом элементе протокола ДОЛЖЕН интерпретироваться как «пробел нулевой ширины без переноса». Если протокол НЕ запрещает использование U+FEFF в качестве подписи для определённого элемента протокола, то реализации ДОЛЖНЫ быть готовы обрабатывать подпись в этом элементе и реагировать соответствующим образом: использовать подпись для определения кодировки символов, если это необходимо, и удалять или игнорировать подпись в зависимости от ситуации.
\newpage
\subsection{ASCII}
Кодовая таблица ASCII — один из наиболее фундаментальных стандартов кодирования символов в вычислительной технике и телекоммуникациях. До её появления каждое устройство (телетайп, компьютер, терминал) могло использовать собственный способ кодирования букв, цифр и знаков, что затрудняло совместимость. Стандарт ASCII был впервые опубликован в 1963 году (ASA X3.4-1963) и затем пересмотрен и расширен. Его целями были: унификация представления символов, повышение совместимости оборудования и программ, упрощение обработки текстовых данных. 

До ASCII существовало множество несовместимых кодировок: на ранних ЭВМ, телетайпах и теле­сетях каждая фирма или система могла применять собственный набор кодов. Работа по стандарту началась примерно в 1961 году, когда инженер Bob Bemer из IBM внёс предложение к ASA (позже — ANSI) о необходимости единого кода. Первое издание стандарта было опубликовано в 1963 году. В последующие годы (1967, 1986) были внесены изменения. (Encyclopedia Britannica В отчёте «Milestones» говорится: «Original commercial use … as a seven-bit teleprinter code … its first commercial use was … Teletype Model 33». Таким образом, ASCII возник как практическое решение проблемы совместимости между устройствами и системами.

Стандарт ASCII — кодировка с длиной 7 бит, что позволяет представить $2^7 = 128$  различных символов. (ASCII Code Эти символы делятся на две большие группы: Контрольные символы (обычно коды 0–31 и 127) — не отображаемые символы, управляющие устройствами или форматированием текста (например, NUL, BEL, LF, CR).  Печатные символы (коды 32–126) — пробел, цифры, латинские буквы (заглавные и строчные), знаки пунктуации и др. Например: код 65 (десятичный) соответствует символу «A». Более подробную таблицу можно найти на тематических ресурсах. Структурно таблицу можно представить как два блока: управляющие символы + графические символы, что упрощает обработку текста и данных.

Стандарт ASCII сыграл ключевую роль в развитии вычислительной техники и информационной обработки: Он стал основой для представления текста в компьютерах, терминалах, принтерах, системах связи, благодаря единому стандарту упростилась передача текстовой информации между разными устройствами и системами, повысилась совместимость, многие последующие кодировки (например, расширенные ASCII-таблицы, затем Unicode/UTF-8) сохранили первые 128 кодов ASCII, обеспечивая обратную совместимость. Применение включает: хранение и обмен текстовых данных, программирование (функции ord()/chr(), указание кодов символов), сетевые протоколы, оформление управляющих символов (табуляция, перевод строки). Например, учебный материал CS50 подчёркивает: «когда компьютер хочет сохранить букву ‘A’, он сохраняет число 65 (1000001 в бинарном)». 

Однако 7 бит позволяют закодировать лишь 128 символов, что недостаточно для многих языков мира, особенно с нелатинскими алфавитами. Появились различные расширенные версии («extended ASCII»), которые используют 8-битовое поле (до 256 символов), но они не стандартизованы универсально. В силу этих ограничений было разработано Unicode, который поддерживает множество языков и символов. ASCII стал лишь базой (первые 128 символов). Таким образом, ASCII можно считать отправной точкой в эволюции кодировок символов.

Хотя ASCII — устаревший в контексте поддержки глобальных наборов символов, он продолжает оставаться предметом исследовательских и прикладных работ: В статье «An Encoding Table Corresponding to ASCII Codes for DNA Data Storage …» авторы используют ASCII-кодировку как основу для кодирования данных в ДНК-хранилищах. В исследовании «An Approach Of Substitution Method Based On ASCII Codes In Encryption Technique» приводится метод шифрования, использующий представление текста через ASCII-коды. Это показывает, что даже основы вроде ASCII продолжают играть роль в новых областях: биохранилища данных, криптография, анализ текстовых форматов.

Кодовая таблица ASCII имеет ключевое историческое и практическое значение: она стала универсальным языком представления текста для компьютеров и телекоммуникаций, повысила совместимость между системами и подготовила почву для более сложных стандартов кодирования. Несмотря на свои ограничения (объём символов, ориентация на латиницу), ASCII остаётся фундаментом и до сих пор используется во многих приложениях. Для современной обработки многоязычных текстов и символов требуется использовать расширенные кодировки (Unicode/UTF-8), но первые 128 символов этих кодировок совпадают с ASCII — это подчёркивает его наследие. Таким образом, понимание структуры, истории и значимости ASCII важно для специалистов по вычислительной технике, программированию и информационным системам.
\newpage
\subsection{Windows‑1251 (CP1251)}
Кодовая страница CP1251 (часто обозначаемая как «Windows-1251») — это 8-битовая однобайтовая кодировка символов, разработанная компанией Microsoft с целью представления текста на языках, использующих кириллицу (в частности, русский, болгарский, украинский и др.). Она расширяет стандартную 7-битную кодировку ASCII (0–127) за счёт использования байтов с кодами от 128 до 255 для дополнительных символов. Таким образом, CP1251 стала одним из ключевых решений для поддержки кириллицы в среде Windows и иной программной среде до широкого распространения Unicode. До появления CP1251 в среде кириллических языков использовались иные кодировки: например, KOI8‑R, ISO/IEC 8859‑5, CP866 и др. Согласно обзору, «CP1251 was invented by Microsoft and ParaGraph (Moscow) as Cyrillic coding for Windows». По историческим данным, она получила широкое применение в русскоязычных версиях Windows и программном обеспечении на базе Windows, особенно в 1990-х — начале 2000-х.

Однако следует отметить, что документация Microsoft по обновлению ANSI-кодовых страниц указывает, что CP1251 фактически не предполагалась к расширению или модификации после её утверждения. Кодировки CP1251 идентичны ASCII в диапазоне 0-127. Для кодов 128-255 назначены символы кириллицы, а также дополнительные типографские и служебные символы. Таблицы CP1251 включают не только базовые кириллические буквы, но иногда и буквы, используемые в украинском, белорусском, сербском языках и др., или вставки типографских символов (например, знак евро) в верхней части диапазона. CP1251 обеспечивала стандартный способ кодирования кириллического текста в среде Windows, что упрощало разработку и совместимость программного обеспечения. Она стала широко распространена в русскоязычных операционных системах, офисных приложениях, базах данных и для обмена текстовыми файлами. В сравнении с более ранними кодировками (например, DOS-ориентированными) CP1251 обеспечивала более «Windows-консистентную» кодировку, что снижало проблемы с отображением текста между приложениями Windows.

Однобайтовая кодировка: CP1251 может содержать максимум 256 символов (0-255). Это логически ограничивает возможность поддержки всех символов кириллицы, особенно исторических, диакритических и прочих расширений. Например, старо-русские буквы, диакритики и др. не всегда корректно кодируются. Совместимость между кодировками: если текст, закодированный в CP1251, обрабатывается в системе с другой кодировкой (например, ISO-8859-5, KOI8-R или UTF-8) без явной конверсии, возможна «кракозябра» (нечитаемый текст). Сегодня CP1251 считается «наследием»: в вебе, особенно, доминирует UTF-8. Например, согласно некоторым данным, лишь около 0.3-0.4 процентов сайтов используют CP1251.

Microsoft отмечает, что ANSI-кодовые страницы (в том числе CP1251) не обновляются и не расширяются. 

CP1251 по-прежнему встречается в наследованных системах, архивных данных и проектах, где нет возможности полностью перейти на Unicode. Однако большинство новых разработок и веб-ресурсов используют универсальные кодировки (например, UTF‑8) из-за их поддержки множества скриптов, символов, языков и меньшей вероятности несовместимости. При миграции данных из CP1251 в современные стандарты важно корректно осуществлять конвертацию, чтобы избежать потери данных или искажений текста.
\newpage
\subsection{KOI8‑R и семейство кодировок KOI8}
Аббревиатура «KOI» происходит от русской фразы «Код Обмена Информацией» («Код Обмена Информацией, 8 бит») — т.е. код для обмена информацией в 8 бит. KOI8 — семейство 8-битовых однобайтовых кодировок символов, разработанных для кириллицы (в частности русского языка) и рассчитанных на компьютерные системы и сети, где использовалась ASCII-совместимая база. Один из наиболее известных вариантов — KOI8-R (русский язык) — широко применявшийся в Unix- и интернет-средах на территории бывшего СССР

В 1974 году был опубликован стандарт GOST 19768‑74, в котором среди прочего была кодировка KOI-8 (8-битный код для обмена и обработки информации). Эта кодировка была создана таким образом, что нижние 128 кодов (0-127) совпадали с ASCII, а верхняя часть (128-255) использовалась для кириллических букв и других графических/спецсимволов. В начале 1990-х годов разработчик Андрей Чернов и другие участники сети «Релком» (Russian Internet) выпустили вариант KOI8-R, зарегистрированный как стандартизированный вариант для русского языка. В результате KOI8-R стала де-факто стандартом кодировки для русского текста в интернет-news, электронной почте и Unix-системах на территории бывшего СССР. 

В KOI8-семействе диапазон 0x00-0x7F (0-127) совпадает с ASCII, что обеспечивает базовую совместимость с латиницей. Верхняя половина диапазона (0x80-0xFF) отводится под кириллические буквы (в KOI8-R — русский алфавит) и дополнительные символы. 

Особенность KOI8: буквы кириллицы расположены не в привычном алфавитном порядке русского алфавита, а так, чтобы при снятии старшего (восьмого) бита результат был прочитываемым латинизированным текстом. Например: «Русский Текст» → rUSSKIJ tEKST при снятом бите. Такая конструкция сделана специально для совместимости с системами, которые могли отбрасывать старший бит или работать только с 7-битными каналами. Для KOI8-R зарегистрирован стандартный идентификатор: кодировка IANA “KOI8-R” (alias csKOI8R) и в Windows — кодовая страница 20866. 

KOI8-R была широко использована для русского языка в ранней сети Интернет: электронная почта, новостные группы (Usenet), сайты, Unix-серверы. Благодаря совместимости с ASCII и особенностям KOI8-кодировки, тексты могли быть частично читаемы даже при несоответствующей дешифровке. Это было практичным в условиях нестабильных систем. 

Хотя KOI8-R поддерживает русский алфавит, она ограничена однобайтовой архитектурой (до 256 кодов) и не покрывает всех кириллических языков, диакритиков, исторических символов. Устройство порядка букв (псевдо-латинский порядок) может представлять неудобство при алфавитной сортировке, лингвистической работе или межъязыковом обмене. С развитием Unicode (UTF-8, UTF-16 и др.) и необходимостью поддержки множества языков, традиционные однобайтовые кодировки (включая KOI8-R) всё чаще замещаются.
\newpage
\subsection{UTF-8, UTF-16, UTF-32}
UTF-8 — поток байтов переменной длины (1–4 байта); обратно-совместим с ASCII, широкоупотребима в интернете и в файловых системах. RFC 3629 — актуальная IETF-спецификация UTF-8 (ограничивает диапазон до U+10FFFF и запрещает суррогатные кодовые единицы в последовательностях UTF-8). 

UTF-8 — формат передачи байтов (сильно детально), переменно-длинный байтовый формат (1–4 байта) для кодирования Unicode code points; совместим с ASCII: кодовые точки U+0000..U+007F кодируются как одни байты с теми же значениями. RFC 3629 описывает формальное ограничение и правила валидности. 

Правила валидности: Нет "overlong" последовательностей: один и тот же символ не должен иметь более длинное представление (напр. символ ASCII не допустимо кодировать 2-/3-/4-байтовыми последовательностями). Overlong — недопустимы и должны считаться ошибками Запрещены surrogate code points (U+D800..U+DFFF) — они существуют только как элементы UTF-16 и не являются законными Unicode code points для кодирования в UTF-8.  Максимум — кодовые точки до U+10FFFF. Любой байтовый поток, нарушающий эти правила, считается некорректным UTF-8. BOM (U+FEFF) как последовательность EF BB BF в UTF-8 допустим, но его применение не рекомендуется как обязательный маркер — RFC и практика веба рассматривают BOM в UTF-8 как спорный.

UTF-8 в итоге экономит место для латиницы и ASCII-типичного текста; для CJK символов UTF-8 часто занимает больше места, чем UTF-16/UTF-32. Современные реализации валидации/транскодирования используют SIMD-приёмы и достигают очень высокого пропускного показателя (billion+ chars/sec) — см. simdutf и статьи/репозитории по SIMD-валидации и транскодированию. Это влияет на выбор: при работе с большими потоками текста критично использовать векторизованные алгоритмы. 

UTF-16 — кодовые единицы 16-битные; для кодовых позиций выше BMP (Basic Multilingual Plane) используются пары суррогатов. Реализации Windows/Java часто оперируют «code units» UTF-16, а не кодовыми точками. UTF-32 — фиксированная длина (32 бита) — простая модель (каждая code point в одной единице), но затратная по памяти; используется редко в качестве внутреннего формата. 

UTF-16 — 16-битные code units и суррогатные пары. UTF-16 представляют текст как последовательность 16-битных code units. Для BMP (Basic Multilingual Plane, U+0000..U+FFFF) обычно хватает одной 16-битной единицы; для кодовых точек выше U+FFFF (U+10000..U+10FFFF) используются суррогатные пары (two 16-bit units: high surrogate + low surrogate). Unicode и платформа Windows/Java долго оперируют UTF-16-единицами как базовой единицей хранения. Непарные суррогаты (single surrogate code unit без пары) считаются ошибкой/некорректным UTF-16 при строгой валидации. 

Поскольку code unit шириной 16 бит, порядок байтов важен: используются LE/BE варианты. BOM (U+FEFF) может быть записан как FE FF или FF FE и служит индикатором порядка байтов — но BOM как маркер встречается редко в сетевых протоколах (UTF-8 чаще). Многие API/языки предоставляют операции «по 16-битным единицам» (например, string.length в некоторых средах), поэтому простые измерения длины или обрезка строк могут порвать суррогатную пару → неправильное отображение/крахи. (Практический совет: оперировать кодовыми точками или графемными кластерами при манипуляциях с пользователем.) UAX \#29 про графемные кластеры важен здесь. 

Нормализация (равнозначность) — зачем нужна и как работает. Одна и та же «внешняя» последовательность символов может иметь разные двоичные представления (например, é = U+00E9 или как U+0065 + U+0301). Unicode описывает четыре формы нормализации (NFC, NFD, NFKC, NFKD) — правила для приведения строк к единому представлению (композиция/декомпозиция, совместимость). Для корректного сравнения строк, индексации, поиска и безопасности часто требуется явная нормализация. См. UAX \#15. 

Текстовая сегментация и «пользовательские символы» Понятие «символа, видимого пользователю» (grapheme cluster / user-perceived character) не совпадает ни с одним из низкоуровневых представлений (байт, code unit, code point). UAX \#29 даёт правила сегментации на графемные кластеры, слова, предложения — важно при подсчёте «символов», перемещении курсора, обрезке строки. 

UTF-32 — фиксированная длина (простая, но расточительная). UTF-32 (часто называют UCS-4) использует 4-байтные (32-битные) code units, каждая такая unit может напрямую содержать одну кодовую точку (обычно в диапазоне 0..0x10FFFF). Преимущества: простая адресация кодовой точки (O(1)), отсутствие суррогатов/переменной длины. Недостаток: высокая память (4 байта на символ), что делает формат редко используемым в хранилищах/сетях, но иногда удобным как внутренний рабочий формат в некоторых приложениях. Опять же, порядок байтов (LE/BE) важен. Спецификация Unicode/ISO описывает UTF-32BE/UTF-32LE. В отличие от UTF-16/UTF-8, UTF-32 держит всегда один код point в одной 32-битной unit, поэтому операции индексирования на уровне кодовых точек тривиальны.

Нормализация, сегментация и безопасность — роли, которые не решают UTF-x. Формат кодирования (UTF-8/16/32) не решает вопрос эквивалентности визуально/семантически одинаковых текстов — это делает нормализация (NFC/NFD/NFKC/NFKD, UAX \#15). Для подсчёта «пользовательских символов» и корректного перемещения курсора нужен алгоритм сегментации (grapheme clusters, UAX \#29). Оба документа — официальные Unicode Annexes и их следует применять поверх низкоуровневого кодирования при сравнениях, фильтрации и отображении. 
\newpage
\section{Особенности проприетарных и открытых форматов текстовых файлов}
\subsection{Проприетарный формат}
Проприетарный формат — формат, созданный и контролируемый обычно частными компаниями.

Некоторые проприетарные форматы могут быть задокументированы разработчиком и опубликованы с пометкой о том, что формат может быть изменён без предварительного уведомления и что файл следует читать или записывать только с помощью библиотек, предоставленных разработчиком. В других случаях спецификация формата кодирования данных может вообще не публиковаться; в некоторых случаях доступ к формату предоставляется только тем, кто подписал соглашение о неразглашении. Проприетарный формат также может представлять собой формат файла, кодировка которого фактически опубликована, но ограничена лицензиями, так что использовать его может только сама компания или лицензиаты.

Особенности проприетарных форматов:
\begin{itemize}
    \item Закрытость — доступ к внутренней структуре файла предоставляется только владельцам или разработчикам, ответственным за создание и поддержку формата.
    \item Совместимость — часто предназначены для работы с собственным программным и аппаратным обеспечением компании.
    \item Лицензирование — использование формата может требовать покупки лицензии или использования продукта компании.
    \item Поддержка — обычно имеют специальную поддержку клиентов и регулярные обновления от компании-владельца.
\end{itemize}

Примеры:

Открытые проприетарные форматы:
\begin{itemize}
    \item {AAC — открытый стандарт, но контролируемый компанией Via Licensing}
    \item GEDCOM — открытая спецификация для обмена генеалогическими данными, контролируемая Церковью Иисуса Христа Святых последних дней
    \item MP3 — открытый стандарт, но в некоторых странах он защищён патентами
\end{itemize}

Закрытые проприетарные форматы:
\begin{itemize}
    \item CDR — (недокументированный) собственный формат CorelDraw, используемый в основном для создания векторных рисунков
    \item DWG — (без документации) чертеж в AutoCAD
    \item MAX — (не задокументировано) 3ds Max
    \item PSD — (документировано[5]) собственный формат изображений Adobe Photoshop
    \item RAR — (частично задокументированный) формат файлов для архивирования и сжатия, разработанный Александром Л. Рошалем
    \item WMA — закрытый формат, контролируемый Microsoft
\end{itemize}
\newpage
\subsection{Открытый формат}
Открытый формат файла — общедоступная спецификация хранения цифровых данных, обычно разрабатываемая некоммерческой организацией по стандартизации, свободная от лицензионных ограничений при использовании. В частности, должна быть возможность включать поддержку открытых форматов как в свободное/открытое, так и в проприетарное ПО, распространяемое по лицензиям, характерным для каждого из этих типов. В отличие от открытых, проприетарные форматы создаются и контролируются обычно частными компаниями и служат их интересам. Открытые форматы являются подмножеством открытых стандартов.

Главная цель открытых форматов — гарантировать возможность доступа к данным в течение долгого времени безо всякой оглядки на лицензионные права и технические спецификации. Другая цель — активизировать конкуренцию вместо того, чтобы позволять компании — автору проприетарного формата препятствовать конкурирующим продуктам. В последние годы правительственные организации многих стран проявляют всё больший интерес к открытым форматам.

Особенности открытых форматов:
\begin{itemize}
    \item Доступность — спецификации формата опубликованы и могут использоваться без ограничений;
    \item Совместимость — предназначены для работы с широким спектром программного и аппаратного обеспечения;
    \item Прозрачность — структура формата открыта, что позволяет любому понять и реализовать её;
    \item Гибкость — можно использовать и адаптировать формат в различных контекстах без проприетарных ограничений
\end{itemize}

Примеры открытых форматов
\begin{itemize}
    \item OASIS OpenDocument Format for Office Applications (формат офисных документов);
    \item LaTeX (язык разметки страниц, используется при подготовке печатных изданий);
    \item DVI (язык описания печатных страниц);
    \item TXT (формат неформатированного текста);
    \item HTML/XHTML (язык разметки web-страниц);
    \item OpenEXR (формат изображений);
    \item PNG (формат изображений);
    \item SVG (формат изображений);
    \item FLAC (аудиоформат);
    \item Контейнер Ogg; Ogg Vorbis (аудиоформат) и Ogg Theora (видео формат);
    \item XML (универсальный язык разметки);
    \item 7z (формат сжатия данных);
    \item CSS;
    \item CSV;
    \item DjVu;
    \item EAS3;
    \item ELF;
    \item FreeOTFE;
    \item Hierarchical Data Format;
    \item iCalendar;
    \item JSON;
    \item LTFS;
    \item NetCDF;
    \item NZB;
    \item PHP;
    \item RSS;
    \item SDXF;
    \item SFV;
    \item TrueCrypt;
    \item WebDAV;
    \item YAML;
\end{itemize}

Архивирование и сжатие:
\begin{itemize}
    \item .7z;
    \item bzip2;
    \item gzip;
    \item PAQ;
    \item Tar;
    \item xz;
    \item .ZIP;
\end{itemize}

Документы:
\begin{itemize}
    \item .HTML;
    \item UTF;
    \item DVI — device independent (TeX);
    \item .EPUB;
    \item LaTeX;
    \item Office Open XML;
    \item .ODT;
    \item OpenXPS;
    \item .PS
\end{itemize}

Мультимедиа:
\begin{itemize}
    \item ALAC;
    \item CMML;
    \item DAISY Digital Talking Book (официально поддерживает аудио только в форматах WAV и платных MP3 и AAC);
    \item FLAC;
    \item JPEG 2000;
    \item .MKV;
    \item MNG;
    \item Musepack;
    \item Ogg;
    \item PNG;
    \item SMIL;
    \item Speex;
    \item SVG;
    \item VRML/X3D;
    \item WavPack;
    \item WebM;
    \item XSPF;
    \item BPG
\end{itemize}
\newpage
\section{Роль языков разметки в структурировании документа}
Языки разметки — это простые правила, по которым в обычный текст добавляют подсказки. Эти подсказки показывают, где начинается заголовок, где идёт абзац, где список, таблица, формула или картинка. Без них текст — просто набор букв, который трудно понять компьютеру. А с ними он становится удобным для машины. Можно быстро превратить его в веб-страницу, PDF, распечатать, отправить в другую программу или даже превратить в аудиокнигу.

Ещё их легко проверять: всё ли на месте, нет ли ошибок в порядке, не забыли ли важные части. Поисковики лучше находят нужное, а люди с плохим зрением могут слушать текст через специальные программы — всё понятно, где что.

Главное — содержание и внешний вид разделены. Текст остаётся тем же, а как он выглядит — меняется отдельно. Например, один и тот же отчёт можно показать на телефоне, на большом экране, напечатать на бумаге или отправить в архив — и ничего не переписывать. Это удобно для людей, для программ и для будущего — документ не устареет, даже если программы сменятся.
\newpage
\subsection{Как всё начиналось}
Всё началось в 1960-х годах в компании IBM. Там работала команда под руководством Чарльза Голдфарба. Они придумали GML — первый способ отмечать части текста, чтобы компьютер понимал, что где. Например: «это заголовок», «это список», «это примечание». Это было нужно, чтобы документы из одной системы можно было перенести в другую без ошибок.

В 1986 году эту идею доработали и сделали международный стандарт SGML (ISO 8879). Это был как конструктор: по его правилам можно было создавать свои системы разметки для разных задач — для книг, технических инструкций, юридических текстов, научных статей.

В 1998 году вышел XML — проще, строже и удобнее для компьютеров. Он стал основой для обмена данными между программами, базами, сайтами.

А для интернета развивался HTML. Первая нормальная версия вышла в 1995 году (HTML 2.0). Потом были 3.2 (1997) и 4.01 (1999) — в них много команд для внешнего вида: цвет текста, шрифт, отступы, рамки.

В 2000 году HTML переделали под строгие правила XML — получился XHTML. Это заставило всех писать аккуратнее: каждый тег должен быть закрыт, всё в правильном порядке.

А в 2014 году вышел HTML5 — современный стандарт. Теперь в нём не только внешний вид, но и смысл: где шапка сайта, где основная статья, где меню, где подвал. Плюс поддержка видео, звуков, анимации, форм, геолокации и работа на телефонах, планшетах, компьютерах.

В России про разметку активно заговорили в 1990-х. Университеты, библиотеки, госучреждения, заводы хотели обмениваться документами. Нужно было, чтобы отчёт из одной программы открывался в другой без ошибок, чтобы данные не терялись. SGML и XML стали основой для таких систем — в налоговой, банках, научных архивах.
\newpage
\subsection{Какие бывают языки разметки}
1.	Для внешнего вида — раньше в HTML писали <font color="red">, <b>, <i>, <center>. Текст становился красным, жирным, по центру. Удобно для глаз, но компьютер не понимал, что важно, а что нет. Это как красивая обложка без содержания.

2.	Для смысла — теперь пишут <strong> (важное), <em> (выделить голосом), <article> (статья), <nav> (меню), <aside> (боковая панель). HTML5 и XML — такие. Компьютер сразу видит: это заголовок, это цитата, это навигация, это примечание.

3. Для узких задач — MathML только для формул (например, $E=mc^2$), SVG — для рисунков и графиков, MusicXML — для нот. LaTeX — для научных текстов: там формулы, ссылки, список литературы, таблицы — всё само считается, нумеруется и красиво выглядит.

4. Простые — Markdown. Пишешь \# Заголовок, \#\# Подзаголовок, —  пункт, `код`, > цитата. Легко читать даже без программы. Потом одним кликом — сайт, PDF, документ Word или презентация.
\newpage
\subsection{Как разметка упорядочивает текст}
1.	Ясные метки — каждый кусок текста получает название. <h1> — главный заголовок, <p> — абзац, <ul> — список с точками, <table> — таблица. Компьютер сразу понимает, что где, и может быстро найти нужное, построить оглавление или выделить главное.

2.	Вложенность — папки в папках. Есть большой раздел, в нём — подраздел, в нём — текст, картинка, формула. Так проще искать, менять, копировать и превращать в другой формат.

3.	Содержание отдельно от вида — текст в одном файле, оформление — в другом. Хочешь PDF для печати — берёшь шаблон с полями и шрифтами. Хочешь сайт — берёшь стили для экрана. Ничего не переписываешь.

4.	Дополнительная информация — можно добавить автора, дату, ключевые слова, лицензию, язык. Например: «автор — Иванов», «тема — экология», «дата — 2025». Удобно искать в архиве, сортировать, показывать только нужное, защищать права.

Примеры

HTML5 — основа всех сайтов. Пример кода:
\begin{verbatim}
<!DOCTYPE html>
<html lang="ru">
<head><title>Моя статья</title></head>
<body>
  <header><h1>Заголовок</h1><p>Кратко о главном</p></header>
  <nav><a href="/">Главная</a> | <a href="/about">О нас</a></nav>
  <main>
    <article>
      <h2>Часть 1</h2>
      <p>Текст здесь... с примерами и пояснениями.</p>
      <figure><img src="pic.jpg" alt="Рисунок">
      <figcaption>Подпись</figcaption></figure>
    </article>
  </main>
  <footer>© 2025, все права защищены</footer>
</body>
</html>
\end{verbatim}
Поисковики видят: где меню, где статья, где картинка. Сайт лучше показывается в поиске. Люди с плохим зрением слушают через программы — всё понятно.

XML — для обмена между программами. В налоговой, банках, больницах, магазинах — всё в XML. Каждый документ проверяется: есть ли номер, дата, подпись, сумма. Если что-то не так — программа сразу говорит: «Ошибка в строке 15».

LaTeX — для науки. Студенты, учёные, журналы — все его используют. Пример:
\begin{verbatim}
\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}

\begin{document}
\title{Моя работа}
\author{Иванов И.И.}
\maketitle

\section{Введение}
Формула: $E = mc^2$. Список литературы в конце.

\begin{references}
\bibitem{einstein} Эйнштейн А. Специальная теория относительности.
\end{references}
\end{document}
\end{verbatim}
Всё само нумеруется: разделы, рисунки, ссылки, страницы. Красиво, чётко, без ошибок.
Markdown — для инструкций и команд. На GitHub, GitLab, в Notion, в Obsidian — везде. Пример:
\begin{verbatim}
# Как установить программу
1. Скачай файл с сайта
2. Распакуй архив
3. Запусти `install.exe`

> Важно: нужен Windows 10 или новее
\end{verbatim}
Потом одним кликом — PDF, сайт, презентация или даже слайды. Удобно, когда работает много людей: один пишет, другой правит, третий публикует.

Плюсы
\begin{itemize}
    \item Компьютер быстро читает — изучает миллионы строк за секунды, ищет, сортирует, анализирует;
    \item Один текст — много форматов — сайт, PDF, печать, аудио, ePub, приложение;
    \item Всё по правилам — меньше ошибок, легче проверять, проще учить новичков;
    \item Удобно для всех — поисковики, слабовидящие, программы, архивы, ИИ;
    \item Долговечность — через 20–30 лет документ откроется, даже если программы сменятся;
\end{itemize}

Минусы
\begin{itemize}
    \item Сложно придумать общие правила — для разных документов нужны разные схемы. Один стандарт не подходит всем;
    \item Многие пишут старым способом — только для вида, а не для смысла. Половина сайтов до сих пор использует <div> вместо <article>;
    \item Нужны программы — для LaTeX — TeX Live или Overleaf, для XML — валидаторы, для Markdown — Pandoc или VS Code;
    \item Учатся не все — новичкам сложно с синтаксисом, отступами, закрытием тегов;
    \item Не всё видно сразу — в Markdown или LaTeX результат видишь только после компиляции;
\end{itemize}

Языки разметки — это каркас всего. Без них текст — просто слова, которые трудно использовать. С ними — удобный, понятный, долговечный документ, который работает везде.

Выбирай по задаче:
\begin{itemize}
    \item Сайт — HTML5;
    \item Наука — LaTeX;
    \item Обмен данными — XML;
    \item Быстро и в команде — Markdown;
\end{itemize}
\newpage
\section{Принципы семантической организации и метаданных текстовых файлов}
Сейчас данных без структуры становится всё больше и больше. Это обычные тексты, офисные документы, страницы в интернете, записи в социальных сетях, электронные письма. Из-за такого роста трудно быстро найти нужное и правильно работать с информацией. Семантическая организация и метаданные помогают соединять данные между собой и понимать их смысл. 
\newpage
\subsection{Основные принципы семантической организации}
Сначала данные строят как дерево. Есть главная папка, в ней подпапки, в них файлы и документы. Это показывает простые связи: «часть внутри целого» или «общее и частное». Например, папка «Компания», внутри «Отделы», а в отделе «Сотрудники». Но для сложных случаев этого мало. Связи становятся запутанными, и поиск не работает.

Онтологии идут дальше и решают проблему. Они описывают разные связи между вещами в одной области. Например: человек работает в компании, документ относится к задаче, событие случилось в городе, продукт продаётся в магазине. Всё это пишут по строгим правилам, чтобы компьютер мог сам читать, понимать и использовать. Онтологии дают полную картину знаний. Они помогают строить системы, где данные не просто лежат, а активно помогают в работе.

В реальной жизни онтологии применяют в медицине. Там описывают болезни, симптомы, лекарства, процедуры. Компьютер может сам предложить диагноз или лечение. 
\newpage
\subsection{Классификация, категоризация и фасетный анализ}
Обычная классификация ставит каждый файл в одну группу и всё. Например: «наука», «бизнес», «личное». Это просто, но не всегда удобно. Фасетный способ добавляет несколько признаков сразу. Каждый признак — это отдельный фильтр.

Признаки могут быть такими:
\begin{itemize}
    \item тема (наука, бизнес, здоровье);
    \item год (2020, 2021, 2022);
    \item автор (Петров, ООО «Ромашка», государственное учреждение);
    \item вид файла (отчёт, презентация, таблица, изображение);
    \item язык (русский, английский, немецкий);
    \item статус (черновик, готово, архив);
\end{itemize}

Теперь можно искать по любому сочетанию. Например: «отчёты по науке за 2021 год на русском языке». Или: «презентации по бизнесу от Петрова». Результат приходит точно и быстро.

Программы машинного обучения сами находят темы в текстах. Они читают много файлов, смотрят слова, фразы, связи. Потом делят всё на группы по смыслу. Например, в папке с новостями одна группа — политика, другая — спорт, третья — культура. Это помогает быстро разбирать большие объёмы. Не нужно читать каждый файл вручную. Программа делает основную работу, а человек только проверяет.
\newpage
\subsection{Семантическое связывание}
Раньше были просто ссылки от одной страницы к другой. Нажал — перешёл. Сейчас связи имеют смысл и тип. Например: «автор написал текст», «город входит в страну».

Для этого берут:
\begin{itemize}
    \item общие номера для объектов (как паспорт для человека);
    \item единые правила записи связей (чтобы все системы понимали одинаково);
\end{itemize}

Получается большая сеть знаний. Файлы из разных мест — компьютер, облако, сервер, интернет — соединяются в одну систему. Можно спрашивать сложное: «покажи всё, что связано с проектом в Москве, где участвует компания из Санкт-Петербурга». Система найдёт путь через связи.

Так работают поисковики. Они не просто ищут слова, а понимают смысл. Спросил «столица Франции» — получили Париж, потому что есть связь «столица — страна». Это экономит время и даёт точные ответы.
\newpage
\subsection{Принципы формирования и стандартизации метаданных}
Чтобы программы из разных мест понимали друг друга, нужны общие описания. Это как единый язык для данных.

\begin{itemize}
    \item Dublin Core — 15 простых полей. Название, автор, дата создания, дата изменения, ключевые слова, описание, формат, источник. Подходит для любых файлов;
    \item Schema.org — большой набор для сайтов. Там описывают товары, события, людей, организации. Поисковики читают это и показывают карточки в выдаче: цена, рейтинг, адрес;
    \item MODS — подробный вариант для библиотек и архивов. Там есть место хранения, размер, материал, права, история изменений;
\end{itemize}

Общие правила делают файлы понятными везде. Один документ можно открыть в библиотечной системе, на сайте, в корпоративной базе — везде он будет выглядеть правильно.
\newpage
\subsection{Автоматическое извлечение и генерация метаданных}
Вручную описывать каждый файл — точно, но долго и дорого. Один человек за день сделает 5–10 штук. А файлов тысячи. Автоматы делают быстрее и дешевле.

Что они умеют:
\begin{itemize}
    \item находить имена людей, фирм, городов, дат;
    \item писать короткие пересказы текста;
    \item определять тип (новость, инструкция, реклама, отчёт);
    \item выделять ключевые слова;
\end{itemize}

Для этого учат специальные модели. Дают миллионы примеров: вот текст, вот правильные метаданные. Модель смотрит контекст, слова вокруг, и решает сама. Со временем она становится точнее.
\newpage
\subsection{Гибридные (полуавтоматические) подходы}
Полностью автомат — быстро, но бывают ошибки. Полностью вручную — точно, но медленно. Лучше смешать.
Смешанный вариант работает так:

1.	Программа читает файл и предлагает описание.

2.	Специалист смотрит только сложные места, где машина не уверена.

3.	Поправки идут обратно в обучение.

Так точность растёт, а времени тратится мало. Программа сама выбирает, где нужна помощь. Это называется активное обучение. Через месяц-два система работает почти без ошибок.

\newpage
\subsection{Продвинутые методы семантической обработки текста}
Слова превращают в числа. Каждая модель даёт слову набор цифр. Похожие по смыслу слова имеют близкие числа.

Модели Word2Vec и GloVe учатся на текстах. Видят, какие слова стоят рядом. «Собака лает», «кошка мяукает» — животные близко. «Стол стоит», «стул стоит» — мебель рядом.

Это помогает искать похожие слова, находить связи. Например, в поиске ввели «автомобиль» — система предложит «машина», «транспорт». В рекомендациях покажет похожие товары.

\newpage
\subsection{Нейросетевые архитектуры для обработки текста}
Нейронные сети решают задачи с текстом лучше старых методов.

Типы сетей:
\begin{itemize}
    \item сверточные — ищут важные куски в предложении;
    \item рекуррентные — помнят, что было раньше в тексте;
    \item трансформеры — смотрят весь текст сразу, с начала и конца;
\end{itemize}

Учат их так: дают текст и правильный ответ. Сеть пробует, ошибается, получает поправку. Повторяют миллионы раз. Потом сеть переводит с языка на язык, отвечает на вопросы, определяет, позитивный текст или негативный.

Например, спросили: «Кто написал письмо?» — сеть найдёт имя, даже если оно в середине текста.

\newpage
\section{Технологии автоматической обработки и анализа текстовой информации}
Технологии автоматической обработки и анализа текстовой информации — это набор методов и инструментов, которые позволяют компьютерам "понимать", интерпретировать и извлекать смысл из неструктурированных текстовых данных.

Эту область также часто называют NLP (Natural Language Processing) или обработка естественного языка.
\newpage
\subsection{Традиционные методы автоматического текстового анализа}
Прежде чем компьютер сможет «понять» и проанализировать текст, его необходимо преобразовать в машиночитаемый формат. Этот процесс лежит в основе всех NLP-технологий.

Токенизация – процесс разбиения текста на отдельные слова или токены. Токенизация является основой для анализа текста и позволяет преобразовать неструктурированный текст в структурированный вид, пригодный для дальнейшего анализа. 

Стемминг и лемматизация – методы приведения слов к их корневой форме. Стемминг основан на удалении окончаний слов, в то время как лемматизация опирается на морфологический анализ и приведение слов к их базовой форме. 

Векторное представление текста – представление текста в виде численного вектора, где каждый элемент вектора соответствует определенному признаку текста. Наиболее известные методы векторного представления текста:

Bag of Words (BoW): Представление текста как неупорядоченного набора слов с указанием их частоты. Не учитывает семантику и порядок слов.

TF-IDF (Term Frequency-Inverse Document Frequency): Усовершенствование BoW, которое оценивает важность слова не только по частоте в документе (TF), но и по его редкости во всей коллекции документов (IDF). Это позволяет отфильтровать общеупотребительные, но маловажные термины.

Кластеризация – разбиение множества документов на кластеры, т. е. подмножества тематически близких документов.
\newpage
\subsection{Ключевые задачи и технологии обработки текста}
На основе подготовленных данных строятся более сложные технологии, решающие прикладные задачи.

Классификация текстов - отнесение документа к одной или нескольким заранее заданным категориям. Применения: спам-фильтрация, определение тональности (сентимент-анализ), рубрикация новостей.

Извлечение именованных сущностей - выявление и классификация именованных сущностей в тексте: имена людей, названия организаций, географические объекты, даты, суммы денег. Используется в информационном поиске, биоинформатике (извлечение названий белков и генов).

Лемматизация и морфологический анализ - определение грамматических характеристик слов (часть речи, падеж, число, время и т.д.).

Определение тональности - одна из самых популярных задач классификации, направленная на выявление эмоциональной окраски текста (позитивный, негативный, нейтральный). Широко используется для анализа отзывов, мониторинга бренда в социальных сетях.

Машинный перевод - автоматический перевод текста с одного языка на другой. Эволюционировал от статистических методов к нейросетевым, которые сегодня демонстрируют высочайшее качество (Google Translate, Yandex Translate).

Суммаризация текста - автоматическое создание краткого содержания исходного текста. Бывает экстрактивной (выбор наиболее важных готовых предложений из текста) и абстрактивной (генерация новых предложений, передающих основную мысль).

Вопросно-ответные системы - системы, которые находят точный ответ на вопрос, заданный на естественном языке, в предоставленном контексте или базе знаний. 

Диалоговые системы (Chatbots и Virtual Assistants) - от простых правиловых ботов до сложных моделей, основанных на генеративных нейросетях (например, ChatGPT). Они способны поддерживать связный диалог, отвечать на вопросы и выполнять команды.
\newpage
\subsection{Методы машинного обучения}
Машинное обучение кардинально изменило подход к анализу текстов, позволив компьютерам не просто следовать жёстким правилам, а самостоятельно находить языковые закономерности в данных. Это открыло возможность решать значительно более сложные задачи, однако эти алгоритмы требуют тщательной подготовки признаков. Они особенно востребованы в задачах классификации текстов, когда важно не только получить результат, но и понять, как он был получен. Их ограничение – необходимость ручного проектирования признаков и сложность работы с контекстными зависимостями.

Наивный Байесовский классификатор – простой и быстрый алгоритм классификации текстов, основанный на теореме Байеса. Этот метод широко используется для фильтрации спама, определения языка текста и категоризации новостей. 

Метод опорных векторов (SVM) – алгоритм машинного обучения, используемый для решения задач классификации и регрессии. SVM особенно эффективен при работе с большим количеством признаков, что делает его подходящим для анализа текстовых данных. 

Решающие деревья и случайный лес (Random Forest) – алгоритмы классификации и регрессии, основанные на построении иерархической структуры решений. Решающие деревья хорошо интерпретируемы и могут использоваться для извлечения правил из текстовых данных, в то время как случайный лес обеспечивает более высокую точность за счет ансамбля деревьев. 
\newpage
\subsection{Методы глубокого обучения}
Нейросетевые архитектуры, такие как CNN для текста, рекуррентные сети (LSTM, GRU) и особенно трансформеры (BERT, GPT), научились автоматически извлекать сложные лингвистические признаки. Эти технологии демонстрируют впечатляющие результаты в задачах, требующих глубокого понимания контекста, таких как машинный перевод или вопросно-ответные системы. Их сила – в способности улавливать сложные языковые паттерны без явного программирования. Однако они требуют огромных объемов данных для обучения и остаются «черными ящиками» с точки зрения интерпретируемости.

Сверточные нейронные сети (CNN) – архитектура нейронных сетей, изначально разработанная для обработки изображений, но также успешно применяемая для анализа текстовых данных. CNN хорошо справляются с задачами классификации текстов и анализа тональности. 

Рекуррентные нейронные сети (RNN) и LSTM – архитектуры нейронных сетей, разработанные специально для работы с последовательными данными, такими как текст. RNN и LSTM способны улавливать контекст и долгосрочные зависимости в тексте, что делает их подходящими для задач генерации текста, машинного перевода и распознавания речи. 

Трансформеры и модели на основе BERT – новые архитектуры нейронных сетей, основанные на механизме внимания (attention). Трансформеры обеспечивают высокую эффективность и масштабируемость при работе с большими объемами текстовых данных, а модели на основе BERT показывают впечатляющие результаты во многих задачах автоматического текстового анализа, таких как извлечение информации, анализ тональности и вопросно-ответные системы. 
\newpage
\section{Основы долговременного хранения и архивации текстовых ресурсов}
Представьте ситуацию: молодожены в 2005 году записали все фото и видео со свадьбы на DVD-диск. Это было модно, удобно — все гости могли посмотреть красивый фильм о торжестве. Диск бережно хранился в шкафу, как семейная реликвия.

Прошло 15 лет. Родился ребенок, вырос, захотел посмотреть, как мама с папой женились. Достали тот самый диск... а оказалось, что:
\begin{itemize}
    \item В доме уже 10 лет как нет DVD-привода — ни в компьютере, ни у телевизора;
    \item Файлы были записаны в формате VOB — современные видеоплееры его не открывают;
    \item Сам диск местами поцарапан и некоторые фрагменты не читаются;
    \item Даже если найти где-то старый DVD-плеер, нужна ещё программа для конвертации формата;
\end{itemize}

И вот уже вместо радостного просмотра — головная боль: как восстановить, переконвертировать, найти технику. А самое обидное — понимание, что уникальные кадры могли быть потеряны навсегда.
Именно так и происходит цифровое старение. Это не про то, что документы пожелтели на полке, а про то, что:
\begin{itemize}
    \item Носители устаревают (дискеты, диски, флешки старого образца);
    \item Форматы становятся неподдерживаемыми (как тот же VOB);
    \item Программы для чтения исчезают;
    \item Оборудование перестаёт выпускаться;
\end{itemize}

И это касается не только видео со свадьбы, но и важных документов: дипломных работ, договоров, налоговых отчётов, семейных архивов.
\newpage
\subsection{Основные проблемы долговременного хранения}
\noindent\textbf{Технологическое старение}

Технологическое старение проявляется в устаревании форматов файлов и программного обеспечения, что приводит к невозможности чтения и использования сохраненных документов. Данная проблема возникает вследствие постоянного развития информационных технологий и смены поколений программных продуктов.

\noindent\textbf{Информационное старение}

Информационное старение характеризуется потерей актуальности и ценности данных с течением времени. Этот процесс обусловлен накоплением новой информации, которая уточняет, дополняет или опровергает ранее существовавшие сведения.

\noindent\textbf{Физическое старение}

Физическое старение выражается в износе и деградации носителей данных. Магнитные, оптические и электронные носители информации имеют ограниченный срок службы, что создает риск безвозвратной потери данных.

\noindent\textbf{Организационно-правовые пробелы}

Отсутствие единых стандартов и нормативов хранения цифровых документов создает правовые и организационные сложности. Недостаточная нормативная база затрудняет создание унифицированных систем долговременного хранения.
\newpage
\subsection{Стратегии долговременного хранения}

\subsubsection{Основные принципы}

Система долговременного хранения должна базироваться на пяти ключевых принципах:

\begin{itemize}
    \item Целостность данных — обеспечивает сохранение подлинности и неизменности документа на протяжении всего периода хранения; данный принцип требует реализации механизмов контроля целостности и защиты от несанкционированных изменений;
    \item Доступность — гарантирует возможность получения stored материалов авторизованными пользователями в установленные сроки; реализация этого принципа предполагает создание надежных систем хранения и эффективных механизмов поиска;
    \item Конфиденциальность — обеспечивает защиту информации от несанкционированного доступа; данный принцип требует применения соответствующих средств криптографической защиты и разграничения прав доступа;
    \item Оригинальность — позволяет подтвердить подлинность и происхождение документа; для реализации этого принципа используются электронные подписи, метки времени и другие средства аутентификации;
    \item Управление жизненным циклом — предусматривает контроль всех этапов существования документа: создание, использование, хранение и удаление; этот принцип требует разработки четких правил и процедур управления документами;
\end{itemize}
\newpage
\subsection{Методы долговременного хранения}

\noindent\textbf{Консервация}

Метод консервации предполагает хранение файлов в исходных форматах и средах. Данный подход требует сохранения оригинального программного обеспечения и технических средств, что создает определенные сложности в долгосрочной перспективе.

\noindent\textbf{Эмуляция}

Эмуляция заключается в воссоздании старого программного окружения на современных платформах. Этот метод позволяет обеспечить доступ к устаревшим форматам данных без их преобразования.

\noindent\textbf{Инкапсуляция}

Инкапсуляция предусматривает включение метаданных и описаний внутрь цифрового объекта. Данный подход обеспечивает сохранение контекстной информации, необходимой для понимания и использования документа.

\noindent\textbf{Миграция}

Миграция представляет собой перенос данных на новые носители и в современные форматы. Регулярное проведение миграции позволяет адаптировать архивные материалы к изменяющимся технологическим условиям.

\noindent\textbf{Цифровая археология}

Цифровая археология направлена на восстановление устаревших или поврежденных данных. Этот метод применяется в случаях, когда стандартные процедуры доступа к информации невозможны.

\noindent\textbf{Форматное нормирование}

Форматное нормирование предполагает использование стандартных открытых форматов (TXT, XML, PDF/A). Данный подход снижает зависимость от конкретных программных продуктов и упрощает долговременное хранение.
\newpage
\subsection{Технологическая адаптивность}

\noindent\textbf{Использование открытых форматов}

Применение открытых форматов обеспечивает независимость от проприетарных программных решений. Открытые стандарты документированы и могут быть реализованы в различных программных средах.

\noindent\textbf{Регулярная миграция} 

Плановое обновление носителей и форматов должно осуществляться с периодичностью не реже чем раз в 5 лет. Данная процедура позволяет предотвратить технологическое устаревание архивных материалов.

\noindent\textbf{Сохранение метаданных и контекста}

Обеспечение сохранности метаданных и контекстной информации является необходимым условием meaningful использования архивных документов. Метаданные должны включать информацию о происхождении, структуре и семантике документов.

\noindent\textbf{Эмуляция ПО}

Сохранение программного обеспечения или его виртуальных аналогов позволяет обеспечить доступ к документам, созданным в устаревших форматах. Эмуляция особенно важна для сохранения интерактивных и мультимедийных материалов.

\noindent\textbf{Организационно-управленческие меры}

Единые стандарты и требования Разработка единых стандартов и требований к форматам, структуре и порядку передачи документов в государственные архивы является основой создания унифицированной системы долговременного хранения. 

\noindent\textbf{Юридическая значимость}

Обеспечение юридической значимости электронных документов требует использования квалифицированной электронной подписи и меток времени. Данные меры позволяют установить подлинность и время создания документа.

\noindent\textbf{Безопасность хранения}

Регламентация доступа и защита от кибератак являются обязательными условиями обеспечения сохранности цифровых архивов. Система безопасности должна включать многоуровневую защиту и регулярный аудит.

\noindent\textbf{Создание специализированных архивов}

Формирование профессиональных центров с квалифицированными кадрами и современной технической базой позволяет обеспечить надлежащие условия долговременного хранения цифровых материалов.

\noindent\textbf{Управление ценностью информации}

\noindent\textbf{Классификация документов}

Разделение документов по значимости (исторические, правовые, научные) позволяет оптимизировать ресурсы хранения и обеспечить дифференцированный подход к сохранности различных категорий материалов.

\noindent\textbf{Динамика ценности}

Учет изменения важности информации во времени необходим для принятия обоснованных решений о сроках и условиях хранения. Ценность документов может изменяться непредсказуемым образом.

\noindent\textbf{Актуализация данных}

Обновление и объединение устаревших сведений позволяет поддерживать релевантность архивных материалов. Процедуры актуализации должны проводиться в установленном порядке.

\noindent\textbf{Оптимизация архивов}

Исключение полностью неактуальных данных способствует рациональному использованию ресурсов хранения. Критерии и процедуры отбора документов на удаление должны быть четко определены.
\newpage
\subsection{Кадровое обеспечение}

\noindent\textbf{Информационные технологии}

Специалисты должны обладать компетенциями в области работы с цифровыми форматами и системами хранения данных. Знание современных информационных технологий является обязательным требованием.

\noindent\textbf{Архивное дело и документоведение}

Понимание принципов систематизации и хранения документов необходимо для организации эффективного архивного делопроизводства. Классические архивные знания должны быть адаптированы к цифровой среде.

\noindent\textbf{Кибербезопасность}
Защита данных от потерь и несанкционированного доступа требует специальных знаний в области информационной безопасности. Специалисты должны владеть современными методами и средствами защиты информации.

\noindent\textbf{Стандартизация и цифровая консервация}

Применение единых методик и норм хранения обеспечивает совместимость и взаимодополняемость различных архивных систем. Знание международных и национальных стандартов является необходимым условием профессиональной деятельности.
\newpage
\subsection{Оборудование и программное обеспечение}

\noindent\textbf{Оборудование для хранения}

\begin{itemize}
    \item Современные системы хранения включают различные типы носителей, каждый из которых имеет специфические характеристики и области применения;
    \item Облачные хранилища обеспечивают удаленный доступ к данным, но требуют реализации дополнительных мер защиты и резервирования;
    \item Внешние носители (SSD, HDD, флешки) позволяют создавать распределенные системы хранения с несколькими копиями данных на разных устройствах;
    \item Ленточные накопители (LTO) характеризуются долговечностью до 30 лет и используются primarily в профессиональных архивных системах;
    \item NAS-системы обеспечивают централизованное хранение и автоматическое резервное копирование данных;
    \item Оптические диски (M-DISC) представляют собой долговечные носители, специально разработанные для задач архивации;
\end{itemize}

\noindent\textbf{Программное обеспечение}

\begin{itemize}
    \item Архиваторы (7-Zip, PeaZip, WinRAR) используются для сжатия и структурирования файлов, что позволяет оптимизировать использование пространства хранения;
    \item Системы цифровых архивов (Archivematica, DSpace, Greenstone) обеспечивают комплексное решение задач хранения и описания цифровых объектов;
    \item Инструменты контроля целостности (Fixity, Checksum Validator) позволяют проводить регулярную проверку стабильности и неизменности данных;
    \item Средства миграции форматов (Pandoc, OpenRefine) используются для конверсии текстов и метаданных в актуальные форматы;
    \item Системы резервного копирования (Duplicati, Restic, BorgBackup) автоматизируют процессы создания резервных копий и обеспечения отказоустойчивости;
\end{itemize}
\newpage
\section{Список упорно трудившихся}
\subsection{Представление и кодирование текстовых данных в цифровой форме}
Морозова Ирина — поиск информации, спикер

Корикова Арина — Markdown

\subsection{Особенности проприетарных и открытых форматов текстовых файлов}
Гайворонская Анжелика — .bib, Markdown, поиск информации

Зайцева Вероника — поиск информации

Бирюкова Анастасия — презентация

Балцевич Артемий — спикер

\subsection{Роль языков разметки в структурировании документа}

Безруких Алиса — поиск информации

Бочанова Олеся — презентация, Markdown

Гинтер Милена — спикер

Корюкова Анастасия — поиск информации
\subsection{Принципы семантической организации и метаданных текстовых файлов}
Балцевич Артемий — спикер

Бочанова Олеся — Markdown

Безруких Алиса — поиск информации

Васильева Дарья — презентация, поиск информации

Гинтер Милена — поиск информации

Манцаева Санжирма — поиск информации
\subsection{Технологии автоматической обработки и анализа текстовой информации}
Аркадьева Ксения — спикер

Карасова Вероника — поиск информации

Идрисова Самира — Markdown

Аксаментова Елена —  презентация
\subsection{Основы долговременного хранения и архивации текстовых ресурсов}
Махмудов Эльвин — поиск информации, Markdown

Кичко Елисей — поиск информации, презентация

Бобкова Вероника — поиск информации, .bib
\subsection{Отдельно отметим}
Бобылев Максим — разработка дизайна для презентаций

Лепешонок Анастасия — LaTeX

Кононов Денис — мейнтейнер

Варлаганова Виктория — мейнтейнер
\newpage
\section*{Список использованных источников}
\addcontentsline{toc}{section}{Список использованных источников}
\nocite{*}  
\bibliographystyle{plainurl}
\bibliography{report}
\end{document}
